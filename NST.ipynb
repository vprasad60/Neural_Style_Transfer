{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/Modify VGG and Establish Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the model and features\n",
    "def model_selection():\n",
    "    # Import VGG-19 network\n",
    "    vgg19 = models.vgg19(pretrained = True).features.to(device).eval()\n",
    "    \n",
    "    # Adjust modules\n",
    "    for name, module in vgg19.named_modules():\n",
    "        # Convert max pooling to average\n",
    "        if isinstance(module, nn.MaxPool2d):\n",
    "            vgg19[int(name)] = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "    # Prevent gradient change\n",
    "    for param in vgg19.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Return modified vgg19\n",
    "    return vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain style and content features\n",
    "def get_features(image, model, layers=None):\n",
    "    # Set feature dictionary \n",
    "    features = {}\n",
    "    x = image\n",
    "    \n",
    "    # Add outputs of layer if they are content or style layers\n",
    "    for name, layer in enumerate(model):\n",
    "        x = layer(x)\n",
    "        if str(name) in layers:\n",
    "            features[str(name)] = x\n",
    "    \n",
    "    # Return list of features\n",
    "    return list(features.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss - gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gram matrix\n",
    "def gram_matrix(tensor):\n",
    "    # Obtain tensor dimensions (batch, channels, height, width)\n",
    "    b, c, h, w = tensor.size()\n",
    "    \n",
    "    # Reshape the tensor to obtain feature space\n",
    "    feature_space = tensor.view(b, c, h*w)\n",
    "    \n",
    "    # Compute gram matrix\n",
    "    gram = torch.bmm(feature_space, torch.transpose(feature_space, 1, 2))\n",
    "    \n",
    "    # Return normalized gram matrix\n",
    "    return gram.div_(h*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Process and Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "def load_images(style_name, content_name):\n",
    "    style_img = Image.open('Style_Images/'+ style_name +'.jpg')\n",
    "    content_img = Image.open('Content_Images/'+ content_name +'.jpg')\n",
    "    \n",
    "    return style_img, content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to convert the image to a tensor\n",
    "def img_to_tensor(image, image_size = 512):\n",
    "    # Create resize and tensor transformation \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),])\n",
    "    # Transform image\n",
    "    tensor = transform(image)\n",
    "    # Add batch dimension and return tensor\n",
    "    return tensor.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to image to confirm resizing\n",
    "def tensor_to_img(tensor):\n",
    "    # Clone tensor\n",
    "    tensor_clone = tensor.cpu().clone()\n",
    "    # Remove batch dimension\n",
    "    tensor_clone = tensor_clone.squeeze(0)\n",
    "    # Convert to image\n",
    "    image = transforms.ToPILImage()(tensor_clone)\n",
    "\n",
    "    # Display image\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nst_image(tensor, content_img = None, style_img = None):\n",
    "    # Create filename\n",
    "    filename = content_img + '_' + style_img + '.jpg'\n",
    "    \n",
    "    # Create directory for saving generated images using content image name\n",
    "    directory = 'Generated_Images/' + content_img\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Save image and print confirmation\n",
    "    save_image(tensor, os.path.join(directory, filename), normalize = True)\n",
    "    print('Transferred Image Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NST Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish variables\n",
    "iters = 25\n",
    "alpha = 10\n",
    "beta = 1e4\n",
    "img_size = 512\n",
    "model = model_selection()\n",
    "content_img = 'neckarfront'\n",
    "style_img = 'haystacks'\n",
    "input_img = 'content'\n",
    "content_layers = ['21']\n",
    "style_layers = ['0','5','10','19','28']\n",
    "style_weights = [0.2, 0.2, 0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run the Neural Style Transfer Algorithm using the following parameters:\n",
    "\n",
    "INPUTS\n",
    "iters: number of iterations using LBFGS loss \n",
    "alpha: content weight\n",
    "beta: style weight\n",
    "img_size: dimensions of resized images (set to 512x512)\n",
    "content_img: image used to extract content features\n",
    "style_img: image used to extract style features\n",
    "input_img: designate input image as 'noise', 'content', or 'style'\n",
    "content_layers: layers of network from which content features are extracted\n",
    "style_layers: layers of network from which style features are extracted\n",
    "style_weights: weights assigned to each style layer (default 0.2 for each)\n",
    "\n",
    "OUTPUT\n",
    "\n",
    "'''\n",
    "def run_nst(iters = iters, alpha = alpha, beta = beta, img_size = img_size, model = model, \n",
    "            content_img = content_img, style_img = style_img, input_img = input_img,\n",
    "            content_layers = content_layers, style_layers = style_layers, style_weights = style_weights):\n",
    "    # Load images\n",
    "    style, content = load_images(style_img, content_img)\n",
    "    \n",
    "    # Convert images to tensors\n",
    "    style_tensor = img_to_tensor(style)\n",
    "    content_tensor = img_to_tensor(content)\n",
    "    \n",
    "    # Obtain style representation \n",
    "    style_rep = get_features(style_tensor, model, style_layers)\n",
    "    \n",
    "    # Convert style features to gram matrices\n",
    "    style_gram = [gram_matrix(style).detach() for style in style_rep]\n",
    "\n",
    "    # Obtain content representation\n",
    "    content_rep = get_features(content_tensor, model, content_layers)[0].detach()\n",
    "    \n",
    "    # Create input image tensor\n",
    "    if input_img == 'content':\n",
    "        noise = content_tensor.clone().requires_grad_().to(device)\n",
    "    elif input_img == 'style':\n",
    "        noise = style_tensor.clone().requires_grad_().to(device)\n",
    "    else:\n",
    "        noise = torch.randn(content_tensor.size(), requires_grad = True, device = device)\n",
    "    \n",
    "    # Set optimizer\n",
    "    optimizer = optim.LBFGS([noise])\n",
    "    \n",
    "    # Run model\n",
    "    for i in tqdm(range(iters)):\n",
    "\n",
    "        def closure():\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialize content and style loss\n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "\n",
    "            # Calculute content features of noise\n",
    "            noise_content = get_features(noise, model, content_layers)[0]\n",
    "\n",
    "            # Calculate content loss\n",
    "            content_loss = nn.MSELoss()(noise_content, content_rep)\n",
    "\n",
    "            # Calculate style features of noise\n",
    "            noise_style = get_features(noise, model, style_layers)\n",
    "\n",
    "            # Convert style features to gram matrices\n",
    "            noise_gram = [gram_matrix(style) for style in noise_style]\n",
    "\n",
    "            # Calculate loss for each gram matrix\n",
    "            for j in range(len(noise_gram)):\n",
    "                style_loss += nn.MSELoss()(noise_gram[j], style_gram[j])*style_weights[j]\n",
    "\n",
    "            # Calculate total loss\n",
    "            total_loss = alpha*content_loss + beta*style_loss \n",
    "            total_loss.backward()\n",
    "\n",
    "            # Backpropagate\n",
    "            return total_loss\n",
    "\n",
    "        # Update\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [02:26<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred Image Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run NST algorithm\n",
    "transfer = run_nst()\n",
    "# Save the image\n",
    "save_nst_image(transfer, content_img = content_img, style_img = style_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_env]",
   "language": "python",
   "name": "conda-env-pytorch_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
